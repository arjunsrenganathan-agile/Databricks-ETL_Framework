{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189618d2-b4dc-4bd2-95fe-73c090a1a271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_bronze_table_config(table_name):\n",
    "    # Read from the control table for bronze configurations\n",
    "    config_df = spark.sql(f\"\"\"\n",
    "        SELECT source_system, bronze_table_name, load_type, file_location\n",
    "        FROM bronze_control_table\n",
    "        WHERE bronze_table_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    return config_df.collect()[0] if not config_df.isEmpty() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ebb023-ba2a-472b-a70b-7620cb26dc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_to_table(table_name, layer, log_message, error_flag=False):\n",
    "    timestamp = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO logging_table (table_name, layer, log_message, log_timestamp, error_flag)\n",
    "        VALUES ('{table_name}', '{layer}', '{log_message}', '{timestamp}', {error_flag})\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e46ab2a-71eb-4567-b127-dfdc1a2dde94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_to_table(table_name, layer, log_message, error_flag=False):\n",
    "    timestamp = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO logging_table (table_name, layer, log_message, log_timestamp, error_flag)\n",
    "        VALUES ('{table_name}', '{layer}', '{log_message}', '{timestamp}', {error_flag})\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a65157d-0e54-4a69-a0ef-b883af1aabbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(file_location):\n",
    "    return spark.read.format(\"parquet\").load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7a20d4-9169-4731-b4b3-2543845268af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_to_bronze_table(df, bronze_table_name):\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67cd84e9-883e-42ae-9ba2-ab5cb17964cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get bronze table names from the tracking table\n",
    "def get_bronze_table_names():\n",
    "    bronze_tables_df = spark.sql(\"\"\"\n",
    "        SELECT table_name \n",
    "        FROM tracking_table \n",
    "        WHERE layer = 'Bronze'\n",
    "    \"\"\")\n",
    "    return [row.table_name for row in bronze_tables_df.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a74e25e-4ef3-454c-817a-0a1f8e1a4c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_tracking_table(table_name, layer):\n",
    "    timestamp = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO tracking_table AS t\n",
    "        USING (SELECT '{table_name}' AS table_name, '{layer}' AS layer, '{timestamp}' AS last_loaded) AS s\n",
    "        ON t.table_name = s.table_name AND t.layer = s.layer\n",
    "        WHEN MATCHED THEN\n",
    "            UPDATE SET last_loaded = s.last_loaded\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT (table_name, layer, last_loaded) VALUES (s.table_name, s.layer, s.last_loaded)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf75ecd-a79f-4db3-85f7-99ab1bd1c9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the transformation function\n",
    "dbutils.notebook.run(\"/Workspace/Metadata_Driven_ETL_Framework/Transformation_Logic/Transform_Bronze_Layer\", 600)\n",
    "\n",
    "def execute_bronze_tasks(table_names):\n",
    "    \"\"\"\n",
    "    Execute ETL tasks for bronze tables.\n",
    "\n",
    "    Parameters:\n",
    "    table_names (list): A list of bronze table names to process.\n",
    "    \"\"\"\n",
    "    for table_name in table_names:\n",
    "        config = get_bronze_table_config(table_name)  # Function to get bronze table config\n",
    "        if config:\n",
    "            source_system = config.source_system\n",
    "            bronze_table_name = config.bronze_table_name\n",
    "            load_type = config.load_type\n",
    "            file_location = config.file_location\n",
    "            \n",
    "            try:\n",
    "                # Extract data\n",
    "                df = extract_data(file_location)\n",
    "                log_to_table(bronze_table_name, \"Bronze\", f\"Successfully extracted data from {file_location}\")\n",
    "\n",
    "                # Transform data\n",
    "                transformed_df = transform_data(df)  # Adjust if you have specific transformations for bronze\n",
    "                log_to_table(bronze_table_name, \"Bronze\", \"Successfully transformed data\")\n",
    "\n",
    "                # Load data into bronze table\n",
    "                load_to_bronze_table(transformed_df, bronze_table_name)  # Function to load data into bronze\n",
    "                log_to_table(bronze_table_name, \"Bronze\", \"Successfully loaded data into bronze table\")\n",
    "\n",
    "                # Update tracking table\n",
    "                update_tracking_table(bronze_table_name, \"Bronze\")\n",
    "                log_to_table(bronze_table_name, \"Bronze\", \"Successfully updated tracking table\")\n",
    "\n",
    "            except Exception as e:\n",
    "                log_to_table(bronze_table_name, \"Bronze\", f\"Error processing {table_name}: {e}\", error_flag=True)\n",
    "                print(f\"Error processing {table_name}: {e}\")\n",
    "\n",
    "        else:\n",
    "            log_to_table(table_name, \"Bronze\", f\"No configuration found for table: {table_name}\", error_flag=True)\n",
    "            print(f\"No configuration found for table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb978792-12e9-4354-8d9b-47f9f096ef90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the list of bronze table names\n",
    "    bronze_table_names = get_bronze_table_names()\n",
    "        \n",
    "    # Execute the bronze tasks\n",
    "    execute_bronze_tasks(bronze_table_names)\n",
    "except Exception as e:\n",
    "    log_to_table(\"Bronze Execution\", \"Bronze\", f\"Error during bronze table operations: {str(e)}\", error_flag=True)\n",
    "    print(f\"Error during bronze table operations: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Build_Bronze_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
